{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "843c3faf-8488-4126-854f-504e1cf3a60e",
   "metadata": {},
   "source": [
    "# Assignment 1: Data Pre-Processing of Historical Tropical Cyclone Records\n",
    "In this assignment, you will explore how to do data exploration and pre-processing with a .csv file containing a global collection of \n",
    "tropical cyclone records, the International Best Track Archive for Climate Stewardship [(IBTrACS)](https://www.ncei.noaa.gov/products/international-best-track-archive). The column variable descriptions are given [here](https://www.ncei.noaa.gov/sites/g/files/anmtlf171/files/2025-04/IBTrACS_v04r01_column_documentation.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d4cd35-3d44-445a-aaa2-ed34082a89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16da69-808c-44cb-b84d-f8ab22e752e4",
   "metadata": {},
   "source": [
    "## Load and aggregate the data set\n",
    "Using the following code, load in the data set (this takes a few seconds to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f31b0018-b41e-4a09-9abf-dae44232e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/csv/ibtracs.ALL.list.v04r00.csv'\n",
    "df = pd.read_csv(url, parse_dates=['ISO_TIME'], usecols=range(12),\n",
    "                 skiprows=[1], na_values=[' ', 'NOT_NAMED'],\n",
    "                 keep_default_na=False, dtype={'NAME': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6126ed-1536-4a64-a9ee-b163a75eb68b",
   "metadata": {},
   "source": [
    "This data set includes cyclone tracks (so it has multiple entries per named cyclone). We'll use the code below to create an aggregated data set for only the named cyclones, which has one entry per cyclone. You will use the data set `dfnamed` for the rest of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "859eca1f-2d71-433e-a3e0-dbbd9727940f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnamed = df.groupby(\"NAME\").agg(MAX_WIND=('WMO_WIND','max'),\n",
    "                               MIN_PRES=('WMO_PRES','min'),\n",
    "                               MEAN_LAT=('LAT','mean'),\n",
    "                               MEAN_LON=('LON','mean'),\n",
    "                               BASIN=('BASIN','first'),\n",
    "                               SUBBASIN=('SUBBASIN','first'),\n",
    "                               NATURE=('NATURE','first'),\n",
    "                               SEASON=('SEASON','first')).reset_index()\n",
    "dfnamed.head()\n",
    "\n",
    "# these lines of code remove the initial dataframe (we won't need it anymore)\n",
    "import gc\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127fa665-6b6d-42bb-9f85-4f066c633981",
   "metadata": {},
   "source": [
    "## Part 1: Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6b91ee-3a44-4f79-9175-241cc5e011ae",
   "metadata": {},
   "source": [
    "How many named cyclones are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bed6f91-335d-446e-bf4c-36de0efc0e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1884"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfnamed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f50e2-e3ae-4c02-b61e-d457748ed30f",
   "metadata": {},
   "source": [
    "1) Use the `pandas` `hist` method to plot the marginal distributions of the variables in the dataframe `dfnamed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc57275-35a1-4241-bc72-f5399c9bf123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb3aa600-e589-44fc-8d08-cf1e9cd8fbb2",
   "metadata": {},
   "source": [
    "2) Use the `seaborn` Pairgrid function to create a scatterplot of all of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0515c1c-d34e-472c-9d03-643d0861132d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f9dd1c4-6669-4d69-bf57-60c3701bfbb5",
   "metadata": {},
   "source": [
    "3) Using `matplotlib`, create a scatter plot of the minimum pressure vs. the maximum wind speed and color by the year the cyclone took place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b28cd-7db7-4d22-b3a2-a4bff4f20e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c348c8a3-fb80-43fc-9fb4-f4ea96a41d17",
   "metadata": {},
   "source": [
    "## Part 2: Handle missing data\n",
    "4) How many non-null values does each variable have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6fc69-332c-4069-a0da-eb5184672761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb57eefe-720e-4998-a2c7-f4de0359fe57",
   "metadata": {},
   "source": [
    "5) Create a new dataframe called `dfdrop` where you have discarded the rows with NaN values in `dfnamed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b83601-419f-4955-8f32-aa0a57c0fe64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f44347-90a0-4b3d-9dc6-a2cc169c8814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8231944f-ed96-4c2d-9c71-5f5bd6b242b0",
   "metadata": {},
   "source": "6. Create a new dataframe called `dfimputed` where you have imputed the missing values in `dfnamed` with 0.0."
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1038b4f3-88cd-466e-a9e2-61ced0404fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4622ab6-9dd8-46bb-a23f-935cdddaf84e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1474aa6-94d0-42c7-94ee-7106c98b92c1",
   "metadata": {},
   "source": [
    "## Part 3: Feature scaling"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5e4706c8d48ecf80"
  },
  {
   "cell_type": "markdown",
   "id": "c34c32fd-c7f7-40a1-a91f-f2ed66bd9f18",
   "metadata": {},
   "source": [
    "7. Scale the `MAX_WIND`using the StandardScaler and put it into an array called `y`, since its the target we want to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa1487e4-a11b-4ab5-b04e-32a19cd12b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "755e58fa-9e02-490d-99f2-681dd0c26962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45434662-657b-46aa-84e8-c9e182a04eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2223820a-99fe-44b7-ba3f-d361fc8b33dd",
   "metadata": {},
   "source": [
    "8. Create a copy of your `dfdrop` dataframe called `dffeatures`, and drop the `NAME` and `MAX_WIND` columns from your `dffeatures` dataframe, since the `MAX_WIND` variable is going to be your target variable and we won't need the cyclone names any longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c74836-2809-4643-83dc-a4da6298f733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "431278cd-afa6-468c-9a59-cda6a9870582",
   "metadata": {},
   "source": [
    "## Part 4: Encode Categorical Variables\n",
    "9) Check which unique categories each of the variables `BASIN`, `SUBBASIN`, and `NATURE` take, using the `dffeatures` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d120587b-484b-40a5-b7da-9d80f82ea96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f7091-214c-497c-bc4c-1debffa3179c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8071a3b6-6292-465a-a08a-01bfa024438a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a389d6de-092a-476e-95c1-c966a6d1e2fc",
   "metadata": {},
   "source": [
    "10. Print out the number of cyclones in each basin and subbasin. Also print out how many of each storm type there is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50149cf1-fa07-4ccb-832f-0633771b1fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ce8ec-eb37-4b8e-95f9-9a760e0cbc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f95dbeb-c653-426c-b30f-f1484f94fb99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0700881a-5940-486f-adea-332660a7b642",
   "metadata": {},
   "source": [
    "11. Encode the `BASIN`, `SUBBASIN`, and `NATURE` variables in `dffeatures` using One Hot Encoding, and standardize `MIN_PRES`, `MEAN_LAT`, and `MEAN_LON`, and `SEASON` using the `StandardScaler`.\n",
    "\n",
    "*Hint*: you can do this in one step using the `sci-kit learn` `ColumnTransformer`. Create an array called `X` that contains the encoded categorical variables and the scaled numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a779364-ccde-4e92-ac6c-c542d156cf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857e3d0-3589-4d64-892b-3f5738c34b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17005f1a-6a1e-4c8c-abca-01d0ef5cccf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1d2b8-fba5-4442-8869-58345e8bc7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b55a36-fbdd-476f-a7ff-23f13d0ed4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e84350-bd8d-4900-af9f-87ee7519f2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11ffa56c-6f41-4b66-8912-51be08f75e7e",
   "metadata": {},
   "source": [
    "12. Print out the feature names associated with the columns in your `X` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e7e7e5-78ef-40f6-83f8-42928612a45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "606380ba-ad80-4c09-86f6-ddd8fb5c4c68",
   "metadata": {},
   "source": [
    "## Part 5: Train, Validation, & Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaccb76-2a4b-45b7-a6f9-aa4f3ca19960",
   "metadata": {},
   "source": "13. Split your data set into a training and test/validation data set using the `train_test_split` function with 80% of your original data for training and 20% for the testing and validation data sets."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8cd41e-8b6c-4492-96b4-e6fbb4360012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a69e78-44f4-4cfe-8eee-5756237c08ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e8f3282-05bd-4925-999e-5d5a36a7d60b",
   "metadata": {},
   "source": "14. Split your `X_test_val` and `y_test_val` again into separate validation and test data sets, that are 10% each of the original data set. Double check that the size of your final training, validation, and test data sets are correct by printing out the shape of each array."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df763cf7-dad6-434c-8ceb-a230139b1e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6adaa-caf2-4fee-a3a4-0e739348bc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a7d0790-4beb-490e-80c6-a0ef25c7a580",
   "metadata": {},
   "source": [
    "15. Save the Training, validation, and test data sets and labels as numpy arrays using np.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525cb77f-69e3-4f80-b4c8-4291f713740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fec90f-743f-4dbc-80ac-71122173f91e",
   "metadata": {},
   "source": [
    "## Part 6: Create your Assignments Repository\n",
    "To turn in this and other homework assignments for this course, you will create an assignments `github` repository.\n",
    "- Create a new directory called `ml4climate2025` in your home directory.\n",
    "- Create a `Readme.md` markdown file that contains your name.\n",
    "- Initialize a new git repository\n",
    "- Add the file and make your first commit\n",
    "- Create a new **private** repository on Github called `ml4climate2025`. (Call it exactly like that. Do not vary the spelling, capitalization, or punctuation.)\n",
    "- Push your `ml4climate2025` repository to Github.\n",
    "- On Github, go to \"settings\"->\"collaborators\", and add `kdlamb` and `ChhaviDixit`\n",
    "- Push new commits to this repository whenever you are ready to hand in your assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5d022-3c6c-4572-8375-41085fb823ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ed0cc-8a9f-4bec-a136-ee19e9320291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a56e9b2-798c-4bb9-a47d-b25139d3bd52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
